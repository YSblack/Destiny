#!/usr/bin/env python3
"""
é«˜è€ƒå¿—æ„¿å¡«æŠ¥ç³»ç»Ÿ - æ•°æ®ç®¡ç†è„šæœ¬
ç”¨äºæ›´æ–°é™¢æ ¡æ•°æ®ã€ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Šã€æ•°æ®ç»´æŠ¤ç­‰
"""

import sys
import os
import argparse
import json
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from models.data_crawler import UniversityDataCrawler, update_university_database
from models.university_data import UniversityDatabase

class DataManager:
    """æ•°æ®ç®¡ç†å™¨"""
    
    def __init__(self):
        self.crawler = UniversityDataCrawler()
        self.db = None
    
    def update_all_data(self):
        """æ›´æ–°æ‰€æœ‰é™¢æ ¡æ•°æ®"""
        print("=" * 60)
        print("å¼€å§‹æ›´æ–°é™¢æ ¡æ•°æ®...")
        print("=" * 60)
        
        try:
            # å¤‡ä»½ç°æœ‰æ•°æ®
            self._backup_existing_data()
            
            # è·å–æœ€æ–°æ•°æ®
            latest_data = update_university_database()
            
            print(f"\nâœ… æ•°æ®æ›´æ–°å®Œæˆï¼")
            print(f"   å…±è·å– {len(latest_data['universities'])} æ‰€é™¢æ ¡æ•°æ®")
            print(f"   å½•å–åˆ†æ•°çº¿è¦†ç›– {len(latest_data['admission_scores'])} æ‰€é™¢æ ¡")
            print(f"   ä¸“ä¸šæ’åæ•°æ® {len(latest_data['majors'])} ä¸ªä¸“ä¸š")
            
            # ç”Ÿæˆæ›´æ–°æŠ¥å‘Š
            self._generate_update_report(latest_data)
            
            return True
            
        except Exception as e:
            print(f"âŒ æ•°æ®æ›´æ–°å¤±è´¥: {e}")
            return False
    
    def _backup_existing_data(self):
        """å¤‡ä»½ç°æœ‰æ•°æ®"""
        backup_file = f"university_data_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        if os.path.exists("university_data.json"):
            try:
                with open("university_data.json", 'r', encoding='utf-8') as f:
                    existing_data = json.load(f)
                
                with open(backup_file, 'w', encoding='utf-8') as f:
                    json.dump(existing_data, f, ensure_ascii=False, indent=2)
                
                print(f"âœ… å·²å¤‡ä»½ç°æœ‰æ•°æ®åˆ°: {backup_file}")
                
            except Exception as e:
                print(f"âš ï¸  å¤‡ä»½æ•°æ®æ—¶å‡ºç°è­¦å‘Š: {e}")
    
    def _generate_update_report(self, data):
        """ç”Ÿæˆæ›´æ–°æŠ¥å‘Š"""
        report = {
            "update_time": datetime.now().isoformat(),
            "statistics": data.get('statistics', {}),
            "data_sources": {
                "universities": len(data['universities']),
                "admission_scores": len(data['admission_scores']),
                "majors": len(data['majors']),
                "rankings": len(data['rankings'])
            },
            "university_breakdown": {
                "985_211": len([u for u in data['universities'] if '985' in u.get('level', '')]),
                "double_first_class": len([u for u in data['universities'] if u.get('is_double_first_class')]),
                "total": len(data['universities'])
            }
        }
        
        report_file = f"update_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        try:
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… æ›´æ–°æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
            
            # æ‰“å°æ‘˜è¦
            print(f"\nğŸ“Š æ•°æ®æ‘˜è¦:")
            print(f"   985/211é™¢æ ¡: {report['university_breakdown']['985_211']} æ‰€")
            print(f"   åŒä¸€æµé™¢æ ¡: {report['university_breakdown']['double_first_class']} æ‰€")
            print(f"   æ€»é™¢æ ¡æ•°é‡: {report['university_breakdown']['total']} æ‰€")
            
        except Exception as e:
            print(f"âš ï¸  ç”ŸæˆæŠ¥å‘Šæ—¶å‡ºç°è­¦å‘Š: {e}")
    
    def validate_data(self):
        """éªŒè¯æ•°æ®å®Œæ•´æ€§"""
        print("=" * 60)
        print("å¼€å§‹éªŒè¯æ•°æ®å®Œæ•´æ€§...")
        print("=" * 60)
        
        try:
            self.db = UniversityDatabase()
            
            issues = []
            warnings = []
            
            # éªŒè¯é™¢æ ¡æ•°æ®
            universities = self.db.universities
            
            if not universities:
                issues.append("æœªæ‰¾åˆ°ä»»ä½•é™¢æ ¡æ•°æ®")
                return False
            
            print(f"âœ… å‘ç° {len(universities)} æ‰€é™¢æ ¡")
            
            # éªŒè¯å¿…è¦å­—æ®µ
            required_fields = ['id', 'name', 'province', 'type', 'level', 'ranking']
            
            for uni in universities:
                for field in required_fields:
                    if field not in uni or uni[field] is None:
                        issues.append(f"é™¢æ ¡ {uni.get('name', 'Unknown')} ç¼ºå°‘å­—æ®µ: {field}")
            
            # éªŒè¯å½•å–åˆ†æ•°çº¿æ•°æ®
            score_coverage = len(self.db.admission_scores)
            if score_coverage < len(universities) * 0.8:
                warnings.append(f"å½•å–åˆ†æ•°çº¿è¦†ç›–ç‡è¾ƒä½: {score_coverage}/{len(universities)}")
            
            # éªŒè¯æ’åæ•°æ®
            rankings = [uni.get('ranking', 999) for uni in universities]
            if len(set(rankings)) != len(rankings):
                warnings.append("å‘ç°é‡å¤çš„é™¢æ ¡æ’å")
            
            # æ‰“å°éªŒè¯ç»“æœ
            if issues:
                print(f"\nâŒ å‘ç° {len(issues)} ä¸ªä¸¥é‡é—®é¢˜:")
                for issue in issues[:10]:  # æœ€å¤šæ˜¾ç¤º10ä¸ª
                    print(f"   - {issue}")
                if len(issues) > 10:
                    print(f"   ... è¿˜æœ‰ {len(issues) - 10} ä¸ªé—®é¢˜")
                return False
            
            if warnings:
                print(f"\nâš ï¸  å‘ç° {len(warnings)} ä¸ªè­¦å‘Š:")
                for warning in warnings:
                    print(f"   - {warning}")
            
            print(f"\nâœ… æ•°æ®éªŒè¯å®Œæˆï¼Œæ€»ä½“çŠ¶å†µè‰¯å¥½")
            return True
            
        except Exception as e:
            print(f"âŒ æ•°æ®éªŒè¯å¤±è´¥: {e}")
            return False
    
    def generate_statistics(self):
        """ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯"""
        print("=" * 60)
        print("ç”Ÿæˆè¯¦ç»†ç»Ÿè®¡ä¿¡æ¯...")
        print("=" * 60)
        
        try:
            if not self.db:
                self.db = UniversityDatabase()
            
            stats = self.db.get_statistics()
            
            print(f"\nğŸ“Š é™¢æ ¡ç»Ÿè®¡:")
            print(f"   æ€»é™¢æ ¡æ•°é‡: {stats.get('total_universities', 0)}")
            
            # æŒ‰å±‚æ¬¡ç»Ÿè®¡
            by_level = stats.get('by_level', {})
            if by_level:
                print(f"\n   æŒ‰åŠå­¦å±‚æ¬¡:")
                for level, count in sorted(by_level.items()):
                    print(f"     {level}: {count} æ‰€")
            
            # æŒ‰ç±»å‹ç»Ÿè®¡
            by_type = stats.get('by_type', {})
            if by_type:
                print(f"\n   æŒ‰é™¢æ ¡ç±»å‹:")
                for type_name, count in sorted(by_type.items()):
                    print(f"     {type_name}: {count} æ‰€")
            
            # æŒ‰çœä»½ç»Ÿè®¡
            by_province = stats.get('by_province', {})
            if by_province:
                print(f"\n   æŒ‰çœä»½åˆ†å¸ƒ (å‰10å):")
                sorted_provinces = sorted(by_province.items(), key=lambda x: x[1], reverse=True)
                for province, count in sorted_provinces[:10]:
                    print(f"     {province}: {count} æ‰€")
            
            # è·å–é¡¶å°–é™¢æ ¡
            top_unis = self.db.get_top_universities(limit=10)
            if top_unis:
                print(f"\n   é¡¶å°–é™¢æ ¡ (å‰10å):")
                for uni in top_unis:
                    print(f"     {uni['ranking']:2d}. {uni['name']} ({uni['province']})")
            
            # åŒä¸€æµé™¢æ ¡
            double_first_class = self.db.get_double_first_class_universities()
            print(f"\n   åŒä¸€æµé™¢æ ¡: {len(double_first_class)} æ‰€")
            
            return True
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return False
    
    def export_data(self, format_type='json'):
        """å¯¼å‡ºæ•°æ®"""
        print("=" * 60)
        print(f"å¯¼å‡ºæ•°æ® (æ ¼å¼: {format_type})...")
        print("=" * 60)
        
        try:
            if not self.db:
                self.db = UniversityDatabase()
            
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            
            if format_type.lower() == 'json':
                filename = f"university_data_export_{timestamp}.json"
                success = self.db.export_data(filename)
                
            elif format_type.lower() == 'excel':
                import pandas as pd
                
                filename = f"university_data_export_{timestamp}.xlsx"
                
                # åˆ›å»ºExcelæ–‡ä»¶
                with pd.ExcelWriter(filename, engine='openpyxl') as writer:
                    # é™¢æ ¡åŸºæœ¬ä¿¡æ¯
                    df_unis = pd.DataFrame(self.db.universities)
                    df_unis.to_excel(writer, sheet_name='é™¢æ ¡ä¿¡æ¯', index=False)
                    
                    # å½•å–åˆ†æ•°çº¿
                    scores_data = []
                    for uni_id, scores in self.db.admission_scores.items():
                        for year, year_scores in scores.items():
                            for subject, score in year_scores.items():
                                scores_data.append({
                                    'university_id': uni_id,
                                    'year': year,
                                    'subject_type': subject,
                                    'score': score
                                })
                    
                    if scores_data:
                        df_scores = pd.DataFrame(scores_data)
                        df_scores.to_excel(writer, sheet_name='å½•å–åˆ†æ•°çº¿', index=False)
                    
                    # ç»Ÿè®¡ä¿¡æ¯
                    stats = self.db.get_statistics()
                    if stats:
                        df_stats = pd.DataFrame([stats])
                        df_stats.to_excel(writer, sheet_name='ç»Ÿè®¡ä¿¡æ¯', index=False)
                
                success = True
                
            else:
                print(f"âŒ ä¸æ”¯æŒçš„å¯¼å‡ºæ ¼å¼: {format_type}")
                return False
            
            if success:
                print(f"âœ… æ•°æ®å¯¼å‡ºæˆåŠŸ: {filename}")
                return True
            else:
                print(f"âŒ æ•°æ®å¯¼å‡ºå¤±è´¥")
                return False
                
        except Exception as e:
            print(f"âŒ å¯¼å‡ºæ•°æ®æ—¶å‡ºé”™: {e}")
            return False
    
    def clean_old_files(self, days=7):
        """æ¸…ç†æ—§æ–‡ä»¶"""
        print("=" * 60)
        print(f"æ¸…ç† {days} å¤©å‰çš„æ—§æ–‡ä»¶...")
        print("=" * 60)
        
        import glob
        from datetime import datetime, timedelta
        
        patterns = [
            "university_data_backup_*.json",
            "update_report_*.json",
            "university_data_export_*.json",
            "university_data_export_*.xlsx"
        ]
        
        cutoff_date = datetime.now() - timedelta(days=days)
        cleaned_count = 0
        
        for pattern in patterns:
            for file_path in glob.glob(pattern):
                try:
                    file_time = datetime.fromtimestamp(os.path.getmtime(file_path))
                    if file_time < cutoff_date:
                        os.remove(file_path)
                        cleaned_count += 1
                        print(f"   åˆ é™¤: {file_path}")
                except Exception as e:
                    print(f"   åˆ é™¤å¤±è´¥ {file_path}: {e}")
        
        print(f"\nâœ… æ¸…ç†å®Œæˆï¼Œåˆ é™¤äº† {cleaned_count} ä¸ªæ–‡ä»¶")
        return True

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='é«˜è€ƒå¿—æ„¿å¡«æŠ¥ç³»ç»Ÿ - æ•°æ®ç®¡ç†å·¥å…·')
    
    parser.add_argument('command', choices=[
        'update', 'validate', 'stats', 'export', 'clean', 'all'
    ], help='æ‰§è¡Œçš„å‘½ä»¤')
    
    parser.add_argument('--format', choices=['json', 'excel'], default='json',
                       help='å¯¼å‡ºæ ¼å¼ (ä»…ç”¨äºexportå‘½ä»¤)')
    
    parser.add_argument('--days', type=int, default=7,
                       help='ä¿ç•™å¤©æ•° (ä»…ç”¨äºcleanå‘½ä»¤)')
    
    args = parser.parse_args()
    
    manager = DataManager()
    
    print(f"\nğŸ“ é«˜è€ƒå¿—æ„¿å¡«æŠ¥ç³»ç»Ÿ - æ•°æ®ç®¡ç†å·¥å…·")
    print(f"æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"å‘½ä»¤: {args.command}")
    
    success = True
    
    if args.command == 'update':
        success = manager.update_all_data()
        
    elif args.command == 'validate':
        success = manager.validate_data()
        
    elif args.command == 'stats':
        success = manager.generate_statistics()
        
    elif args.command == 'export':
        success = manager.export_data(args.format)
        
    elif args.command == 'clean':
        success = manager.clean_old_files(args.days)
        
    elif args.command == 'all':
        success = (
            manager.update_all_data() and
            manager.validate_data() and
            manager.generate_statistics() and
            manager.export_data('json')
        )
    
    if success:
        print(f"\nğŸ‰ å‘½ä»¤ '{args.command}' æ‰§è¡ŒæˆåŠŸï¼")
        return 0
    else:
        print(f"\nğŸ’¥ å‘½ä»¤ '{args.command}' æ‰§è¡Œå¤±è´¥ï¼")
        return 1

if __name__ == '__main__':
    exit(main()) 